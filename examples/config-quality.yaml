# ğŸ’ QUALITY CONFIG - Best Possible Results  
# When you want the highest quality search and AI responses
# Perfect for: learning new codebases, research, complex analysis

#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ QUALITY-OPTIMIZED SETTINGS - Everything tuned for best results!
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ğŸ“ Chunking for maximum context and quality
chunking:
  max_size: 3000          # Larger chunks = more context per result
  min_size: 200           # Ensure substantial content per chunk
  strategy: semantic      # Smart splitting that respects code structure

# ğŸŒŠ Conservative streaming (favor quality over speed)
streaming:
  enabled: true
  threshold_bytes: 2097152  # 2MB - less aggressive chunking

# ğŸ“ Comprehensive file inclusion
files:
  min_file_size: 20       # Include even small files (might contain important info)
  
  # ğŸ¯ Minimal exclusions (include more content)
  exclude_patterns:
    - "node_modules/**"   # Still skip these (too much noise)
    - ".git/**"          # Git history not useful for code search
    - "__pycache__/**"   # Python bytecode
    - "*.pyc"
    - ".venv/**"
    - "build/**"         # Compiled artifacts
    - "dist/**"
    # Note: We keep logs, docs, configs that might have useful context
  
  include_patterns:
    - "**/*"             # Include everything not explicitly excluded

# ğŸ§  Best embedding quality
embedding:
  preferred_method: ollama  # Highest quality embeddings (needs Ollama)
  ollama_model: nomic-embed-text  # Excellent code understanding
  ml_model: sentence-transformers/all-MiniLM-L6-v2  # Good fallback
  batch_size: 16          # Smaller batches for stability

# ğŸ” Search optimized for comprehensive results
search:
  default_top_k: 15       # More results to choose from
  enable_bm25: true       # Use both semantic and keyword matching
  similarity_threshold: 0.05  # Very permissive (show more possibilities)
  expand_queries: true    # Automatic query expansion for better recall

# ğŸ¤– High-quality AI analysis
llm:
  synthesis_model: auto         # Use best available model
  enable_synthesis: true        # AI explanations by default
  synthesis_temperature: 0.4    # Good balance of accuracy and insight
  cpu_optimized: false         # Use powerful models if available
  enable_thinking: true        # Show detailed reasoning process
  max_expansion_terms: 10      # Comprehensive query expansion

#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’ WHAT THIS CONFIG MAXIMIZES:
# 
# ğŸ¯ Search comprehensiveness - find everything relevant
# ğŸ¯ Result context - larger chunks with more information
# ğŸ¯ AI explanation quality - detailed, thoughtful analysis
# ğŸ¯ Query understanding - automatic expansion and enhancement
# ğŸ¯ Semantic accuracy - best embedding models available
# 
# âš–ï¸ TRADE-OFFS:
# â³ Slower indexing (larger chunks, better embeddings)
# â³ Slower searching (query expansion, more results)
# ğŸ’¾ More storage space (larger index, more files included)
# ğŸ§  More memory usage (larger batches, bigger models)
# âš¡ Higher CPU/GPU usage (better models)
# 
# ğŸ¯ PERFECT FOR:
# â€¢ Learning new, complex codebases
# â€¢ Research and analysis tasks  
# â€¢ When you need to understand WHY code works a certain way
# â€¢ Finding subtle connections and patterns
# â€¢ Code review and security analysis
# â€¢ Academic or professional research
# 
# ğŸ’» REQUIREMENTS:
# â€¢ Ollama installed and running (ollama serve)
# â€¢ At least one language model (ollama pull qwen3:1.7b)
# â€¢ Decent computer specs (4GB+ RAM recommended)
# â€¢ Patience for thorough analysis ğŸ˜Š
# 
# ğŸš€ TO USE THIS CONFIG:
# 1. Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh
# 2. Start Ollama: ollama serve
# 3. Install a model: ollama pull qwen3:1.7b
# 4. Copy config: cp examples/config-quality.yaml .mini-rag/config.yaml  
# 5. Index project: ./rag-mini index /path/to/project
# 6. Enjoy comprehensive analysis: ./rag-mini explore /path/to/project
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ğŸ§ª ADVANCED QUALITY TUNING (optional):
# 
# For even better results, try these model combinations:
# â€¢ ollama pull nomic-embed-text:latest  (best embeddings)
# â€¢ ollama pull qwen3:1.7b              (good general model)
# â€¢ ollama pull qwen3:4b                (excellent for analysis)
# 
# Or adjust these settings for your specific needs:
# â€¢ similarity_threshold: 0.3   (more selective results)
# â€¢ max_size: 4000             (even more context per result)  
# â€¢ enable_thinking: false     (hide reasoning, show just answers)
# â€¢ synthesis_temperature: 0.2 (more conservative AI responses)