# FSS-Mini-RAG Configuration
# 
# ðŸ”§ EDIT THIS FILE TO CUSTOMIZE YOUR RAG SYSTEM
# 
# This file controls all behavior of your Mini-RAG system.
# Changes take effect immediately - no restart needed!
# 
# ðŸ’¡ IMPORTANT: To change the AI model, edit the 'synthesis_model' line below
# 
# Common model options:
#   synthesis_model: auto              # Let system choose best available
#   synthesis_model: qwen3:0.6b        # Ultra-fast (500MB)
#   synthesis_model: qwen3:1.7b        # Balanced (1.4GB) - recommended
#   synthesis_model: qwen3:4b          # High quality (2.5GB)
#
# See docs/GETTING_STARTED.md for detailed explanations

# Text chunking settings
chunking:
  max_size: 2000      # Maximum characters per chunk
  min_size: 150       # Minimum characters per chunk
  strategy: semantic    # 'semantic' (language-aware) or 'fixed'

# Large file streaming settings
streaming:
  enabled: true
  threshold_bytes: 1048576  # Files larger than this use streaming (1MB)

# File processing settings
files:
  min_file_size: 50        # Skip files smaller than this
  exclude_patterns:
    - "node_modules/**"
    - ".git/**"
    - "__pycache__/**"
    - "*.pyc"
    - ".venv/**"
    - "venv/**"
    - "build/**"
    - "dist/**"
  include_patterns:
    - "**/*"                  # Include all files by default

# Embedding generation settings
embedding:
  preferred_method: ollama     # 'ollama', 'ml', 'hash', or 'auto'
  ollama_model: nomic-embed-text
  ollama_host: localhost:11434
  ml_model: sentence-transformers/all-MiniLM-L6-v2
  batch_size: 32               # Embeddings processed per batch

# Search behavior settings
search:
  default_top_k: 10           # Default number of top results
  enable_bm25: true             # Enable keyword matching boost
  similarity_threshold: 0.1        # Minimum similarity score
  expand_queries: false          # Enable automatic query expansion

# LLM synthesis and query expansion settings
llm:
  ollama_host: localhost:11434
  synthesis_model: qwen3:1.7b    # 'auto', 'qwen3:1.7b', etc.
  expansion_model: auto     # Usually same as synthesis_model
  max_expansion_terms: 8        # Maximum terms to add to queries
  enable_synthesis: false       # Enable synthesis by default
  synthesis_temperature: 0.3      # LLM temperature for analysis